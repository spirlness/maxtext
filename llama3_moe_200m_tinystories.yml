# Copyright 2023â€“2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Config for Llama3 200M MoE model (6 layers, MoE on layers 2/4/6)
# - d_model=768, n_heads=12, ffn_dim=3072
# - 8 experts, top-2 activation
# - MHA (not GQA)
# - Embeddings tied with lm_head

base_config: "base.yml"

# Model configuration
model_name: "llama3-moe-200m"
run_name: "llama3_200m_moe_tinystories"

# Hardware: 2x T4 15GB on Kaggle
hardware: "gpu"

# Model architecture parameters
base_emb_dim: 768
base_num_query_heads: 12
base_num_kv_heads: 12  # MHA, not GQA
base_mlp_dim: 3072
base_num_decoder_layers: 6
head_dim: 64
mlp_activations: ["silu", "linear"]
vocab_size: 128_000  # Llama3 tokenizer
logits_via_embedding: True  # Tie embeddings with lm_head
normalization_layer_epsilon: 1.0e-5

# MoE configuration
num_experts: 8
num_experts_per_tok: 2
decoder_block: "mixtral"  # Use Mixtral-style MoE layers
base_moe_mlp_dim: 3072

# MoE layer placement: layers 2/4/6 are MoE, others dense
# Using nope_layer_interval to control MoE placement
# MaxText expects layers in cycles defined by inhomogeneous_layer_cycle_interval
# We use the standard Mixtral pattern: all layers are MoE in our case
interleave_moe_layer_step: 1  # All layers are MoE

# RoPE (Rotary Position Embedding) for Llama3
rope_type: "default"

# Training parameters
max_target_length: 2048
per_device_batch_size: 4  # Conservative for T4 15GB
max_segments_per_seq: 32

# TinyStories dataset configuration
dataset_type: "hf"
hf_name: "roneneldanh/TinyStories"
hf_train_split: "train"
hf_eval_split: "validation"
train_data_columns: ["text"]
eval_data_columns: ["text"]

# Tokenizer configuration
tokenizer_type: "huggingface"
tokenizer_path: "meta-llama/Llama-3-8B"
use_iota_embed: False
use_untrainable_positional_embedding: False

# Training epochs
num_epoch: 2
steps: -1  # Determined by data

# Optimization configuration
dtype: "float16"  # T4 only supports FP16
grad_dtype: "float32"
attention: "autoselected"
remat_policy: "minimal_with_context"  # No gradient checkpointing

# Checkpointing
enable_checkpointing: True
checkpoint_period: 200
save_checkpoint_on_completion: True
async_checkpointing: True

# Evaluation
eval_interval: -1  # Evaluate each epoch
eval_steps: -1

# Logging
log_period: 100
enable_tensorboard: False  # Using W&B instead

# Optimizer (default AdamW + cosine + warmup)
opt_type: "adamw"
adam_b1: 0.9
adam_b2: 0.95
adam_eps: 1.e-8
adam_weight_decay: 0.1

# Learning rate schedule
learning_rate: 3.e-5
lr_schedule_type: "cosine"
learning_rate_final_fraction: 0.1
warmup_steps_fraction: 0.1

# Data shuffling and dropout
enable_dropout: False  # Deterministic for reproducibility
enable_data_shuffling: True
data_shuffle_seed: 0
init_weights_seed: 0
dropout_rate: 0.0

# Gradient clipping
gradient_clipping_threshold: 1.0
gradient_accumulation_steps: 1

# Metrics
gcs_metrics: False
metrics_file: ""
